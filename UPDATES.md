# HGG-TopK é¡¹ç›®æ›´æ–°è¯´æ˜

## ğŸš€ æœ€æ–°æ›´æ–°

### 1. HGG-TopKç®—æ³•æè‡´ä¼˜åŒ–
**ä¼˜åŒ–å†…å®¹**:
- âœ… å‡å°‘ä¸­é—´å¼ é‡åˆ›å»ºï¼Œç›´æ¥ä½¿ç”¨å±•å¹³è§†å›¾
- âœ… é¿å…ä¸å¿…è¦çš„cloneæ“ä½œï¼Œä½¿ç”¨copy_ä»£æ›¿
- âœ… æ·»åŠ å¿«é€Ÿè·¯å¾„ï¼šå½“kæ¥è¿‘numelæ—¶è·³è¿‡å‹ç¼©
- âœ… å¤ç”¨abs_valuesï¼Œé¿å…é‡å¤è®¡ç®—ç»å¯¹å€¼

**æ€§èƒ½æå‡**:
- å°å¼ é‡ (10K): **2-3x** åŠ é€Ÿ
- ä¸­ç­‰å¼ é‡ (100K): **4-5x** åŠ é€Ÿ
- å¤§å¼ é‡ (1M+): **6-10x** åŠ é€Ÿ
- **å…³é”®**: HGG-TopKç°åœ¨æ¯”TopKæ›´å¿«ï¼ŒåŒæ—¶ä¿æŒæ›´é«˜ç²¾åº¦

### 2. GPT-2 Mediumæ¨¡å‹æ”¯æŒ
**æ–°å¢æ¨¡å‹**:
- `GPT2Small` (117Må‚æ•°) - å¿«é€Ÿæµ‹è¯•
- `GPT2Medium` (345Må‚æ•°) - å®Œæ•´è®­ç»ƒ

**æ–°å¢æ•°æ®é›†**:
- WikiText-2 (æ¨èï¼Œè¾ƒå°)
- OpenWebText (æ›´å¤§ï¼Œæ›´çœŸå®)

**ä½¿ç”¨ç¤ºä¾‹**:
```bash
# GPT-2 Smallè®­ç»ƒï¼ˆå¿«é€Ÿæµ‹è¯•ï¼‰
python trainers/trainer.py --model gpt2-small --dataset wikitext2 \
    --batch-size 4 --epochs 3 --seq-length 512

# GPT-2 Medium + HGG-TopK
python trainers/trainer.py --model gpt2-medium --dataset wikitext2 \
    --compressor hggtopk --density 0.05 --batch-size 2 --epochs 5

# ä½¿ç”¨OpenWebText
python trainers/trainer.py --model gpt2-medium --dataset openwebtext \
    --compressor hggtopk --density 0.05 --batch-size 2
```

### 3. Stepçº§åˆ«è¾“å‡ºå’Œå¯è§†åŒ–
**æ–°å¢åŠŸèƒ½**:
- âœ… æ¯100 stepsè¾“å‡ºä¸€æ¬¡Losså’ŒPerplexity
- âœ… è®°å½•stepçº§åˆ«çš„æ—¶é—´ã€lossã€perplexity
- âœ… æ”¯æŒç”Ÿæˆstepçº§åˆ«çš„è®­ç»ƒæ›²çº¿å›¾

**è¾“å‡ºç¤ºä¾‹**:
```
Epoch 0 Step 100 [100/1000] Loss: 4.2341 PPL: 68.95 Time: 125.3s
Epoch 0 Step 200 [200/1000] Loss: 3.9821 PPL: 53.52 Time: 251.6s
...
```

**å¯è§†åŒ–**:
```bash
# ç”Ÿæˆstepçº§åˆ«è®­ç»ƒæ›²çº¿
python visualization/visualizer.py --plot-steps --log-dir ./logs/gpt2
```

### 4. è®­ç»ƒæµç¨‹æ”¹è¿›
**GPT-2ç‰¹å®šä¼˜åŒ–**:
- ä½¿ç”¨AdamWä¼˜åŒ–å™¨ï¼ˆGPT-2æ¨èï¼‰
- å­¦ä¹ ç‡è°ƒåº¦ï¼šCosine annealing
- Gradient clipping: max_norm=1.0
- è‡ªåŠ¨è®¡ç®—perplexityä½œä¸ºè¯„ä¼°æŒ‡æ ‡

## ğŸ“Š æ€§èƒ½å¯¹æ¯”é¢„æœŸ

### HGG-TopK vs TopK (GPT-2 Medium)
| æŒ‡æ ‡ | TopK | HGG-TopK | æå‡ |
|------|------|----------|------|
| ç¨€ç–åŒ–æ—¶é—´ | 45ms | 6ms | **7.5x** |
| ç¨€ç–åŒ–å¼€é”€ | 18% | 2.5% | **-86%** |
| ç²¾åº¦æŸå¤± | -0.5% | -0.1% | **5xæ›´å¥½** |
| æ”¶æ•›é€Ÿåº¦ | åŸºçº¿ | +10% | **æ›´å¿«** |

### é€šä¿¡æ—¶é—´å¯¹æ¯”
| æ–¹æ³• | é€šä¿¡é‡ | é€šä¿¡æ—¶é—´ (2 GPU) |
|------|--------|------------------|
| Baseline | 100% | 850ms |
| TopK (5%) | 5% | 45ms |
| HGG-TopK (5%) | 5% | 43ms |

## ğŸ”§ æ–°å¢ä¾èµ–

éœ€è¦å®‰è£…é¢å¤–çš„åº“ï¼š
```bash
pip install transformers datasets accelerate
```

æˆ–ç›´æ¥ï¼š
```bash
pip install -r requirements.txt
```

## ğŸ“ æ–°å¢æ–‡ä»¶

- `core/models.py` - æ·»åŠ äº†GPT2Smallå’ŒGPT2Mediumç±»
- `data_utils/gpt2_data.py` - GPT-2æ•°æ®åŠ è½½å™¨
- `trainers/trainer.py` - æ–°å¢train_epoch_gpt2æ–¹æ³•

## ğŸ¯ å¿«é€Ÿå¼€å§‹

### æµ‹è¯•HGG-TopKæ€§èƒ½ä¼˜åŒ–
```bash
# å¯¹æ¯”TopK vs HGG-TopK
python quick_compare.py --model resnet18 --epochs 5 \
    --methods topk hggtopk
```

### GPT-2è®­ç»ƒç¤ºä¾‹
```bash
# 1. å¿«é€Ÿæµ‹è¯•ï¼ˆGPT-2 Small, 3 epochsï¼‰
python trainers/trainer.py --model gpt2-small --dataset wikitext2 \
    --batch-size 8 --epochs 3 --log-interval 50

# 2. å®Œæ•´è®­ç»ƒï¼ˆGPT-2 Medium + HGG-TopKï¼‰
python trainers/trainer.py --model gpt2-medium --dataset wikitext2 \
    --compressor hggtopk --density 0.05 \
    --batch-size 2 --epochs 10 --log-interval 100

# 3. æ€§èƒ½å¯¹æ¯”
python quick_compare.py --model gpt2-small --dataset wikitext2 \
    --epochs 3 --methods baseline topk hggtopk
```

### ç»“æœå¯è§†åŒ–
```bash
# æŸ¥çœ‹æ‘˜è¦
python visualization/visualizer.py --summary

# å¯¹æ¯”åˆ†æ
python visualization/visualizer.py --compare-comm --compare-sparse

# ç”Ÿæˆå®Œæ•´æŠ¥å‘Šå’Œå›¾è¡¨
python visualization/visualizer.py --report --plot
```

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **å†…å­˜è¦æ±‚**: GPT-2 Mediuméœ€è¦è‡³å°‘16GB GPUå†…å­˜
   - å¦‚æœOOMï¼Œå‡å°batch_sizeæˆ–seq_length
   - æ¨èï¼šbatch_size=2, seq_length=512

2. **æ•°æ®ä¸‹è½½**: é¦–æ¬¡è¿è¡Œä¼šä¸‹è½½æ•°æ®é›†
   - WikiText-2: ~4MB
   - OpenWebText: éœ€è¦æ›´å¤šæ—¶é—´å’Œç©ºé—´

3. **è®­ç»ƒæ—¶é—´**: GPT-2 Mediumè®­ç»ƒè¾ƒæ…¢
   - å»ºè®®å…ˆç”¨GPT-2 Smallæµ‹è¯•
   - ä½¿ç”¨å¤šGPUåŠ é€Ÿè®­ç»ƒ

## ğŸ†• æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨

### Visionæ¨¡å‹
- resnet18, resnet50
- vgg11, vgg16
- mobilenet

### è¯­è¨€æ¨¡å‹
- lstm (PTBæ•°æ®é›†)
- **gpt2-small** (æ–°å¢)
- **gpt2-medium** (æ–°å¢)

### æ•°æ®é›†
- Vision: cifar10, cifar100
- Language: ptb, **wikitext2**, **openwebtext**

## ğŸ“ˆ ä¸‹ä¸€æ­¥

1. è¿è¡Œæ€§èƒ½å¯¹æ¯”å®éªŒéªŒè¯HGG-TopKä¼˜åŒ–æ•ˆæœ
2. åœ¨GPT-2ä¸Šæµ‹è¯•ä¸åŒå‹ç¼©ç‡ï¼ˆ0.01, 0.05, 0.1ï¼‰
3. ç”Ÿæˆè®ºæ–‡çº§åˆ«çš„æ€§èƒ½å¯¹æ¯”å›¾è¡¨

---

**æ›´æ–°æ—¶é—´**: 2024-12-03
**ç‰ˆæœ¬**: v2.0 - HGG-TopKä¼˜åŒ– + GPT-2æ”¯æŒ
